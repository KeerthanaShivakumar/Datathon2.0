# -*- coding: utf-8 -*-
"""Datathon2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OArvh-VB4cJC2J9P6RMT3XXRJtyKPlW-

#Team name - Yukee, Members - Keerthana Shivakumar, Yuktha M

# Round 1
"""

#Importing all libraries necessary for EDA
import pandas as pd
import numpy as np
import seaborn as sns

"""Understanding data"""

from google.colab import files
uploaded = files.upload()

data=pd.read_csv("2019.csv")

data.head()

data.tail()

data.shape

data.describe()

data.columns

data.nunique()

data.info()

#Data cleaning
data.isnull().sum()

"""Use pairplot to plot pairwise relationships in a dataset. Each variable has comparison with every variable helping us to scale down our analysis to a certain level."""

sns.pairplot(data)

"""From the lower graphs, it can be seen that because lower perception of corruption increases freedom of life choices, it increases happiness score

Finding the top 10 happiest countries
"""

top10=pd.DataFrame(data.head(10))
top10[top10.columns[:3]]

"""Finding the 10 least happy countries"""

top10=pd.DataFrame(data.tail(10))
top10[top10.columns[:3]]

"""Finding relation between happiness score and GDP per capita"""

import matplotlib.pyplot as plt

plt.scatter(data['GDP per capita'],data['Score'])

"""Finding relation between happiness score and Social Support"""

plt.scatter(data['Social support'],data['Score'])

"""Finding relation between happiness score and Healthy Life Expectancy"""

plt.scatter(data['Healthy life expectancy'],data['Score'])

"""Finding relation between happiness score and Freedom"""

plt.scatter(data['Freedom to make life choices'],data['Score'])

"""Finding relation between happiness score and Generosity"""

plt.scatter(data['Generosity'],data['Score'])

"""Finding relation between happiness score and Perception of Corruption"""

plt.scatter(data['Perceptions of corruption'],data['Score'])

"""Feature map - finding the extent to which each parameter affects happiness score - Heatmap to indicate relationship between features of data"""

temp = pd.DataFrame(data, columns=['Country or region', 'Score', 'GDP per capita','Social support', 'Healthy life expectancy',
                                   'Freedom to make life choices', 'Generosity','Perceptions of corruption'])
sns.heatmap(temp.corr(),cmap=plt.cm.Reds,annot=True)
plt.figure(figsize=(10,6))
plt.show()

"""In the above heatmap, it can be seen that GDP per capita and Healthy Life Expectancy have a greater positive correlation (0.84). It can be interpreted as, higher GDP per capita indicates higher standards of life and hence, healthy life expectancy.

Histogram of Happiness Scores
"""

plt.figure(figsize=(8,6))
sns.distplot(data['Score'],kde=True,color='blue')
plt.title("Histogram of Happiness Scores",fontsize=17)

"""The happiness scores are normally distributed. Most happiness scores lie between 4 and 7

#Round 2

Create a temporary DataFrame with all numerical values that needs to be fitted (don't modify score because it is a dependent variable) according to StandardScaler transformation
"""

tempDf = pd.DataFrame(data, columns = ['GDP per capita', 'Social support', 'Healthy life expectancy','Freedom to make life choices', 'Generosity', 'Perceptions of corruption'])
from sklearn.preprocessing import StandardScaler
X_std = StandardScaler().fit_transform(tempDf)
#X_std

"""Create a heatmap to understand the top variables (independent) that affect the happiness score"""

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(12, 10))
plt.title('Pearson Correlation of Happiness Index')
# Draw the heatmap using seaborn
sns.heatmap(tempDf.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap="YlGnBu", linecolor='black', annot=True)

"""Standardization causes mean of every feature to become 0 (in this case, near 0) and variance 1. Compute the covariance matrix. Find the Eigenvalues and Eigenvectors"""

mean_vec = np.mean(X_std,axis=0)
#print("mean_vec: ",mean_vec)
cov_mat = np.cov(X_std.T)
#print("cov_mat: ",cov_mat)
eig_vals, eig_vecs = np.linalg.eig(cov_mat)
'''print("eig_vals: ",eig_vals)
print("eig_vecs: ",eig_vecs)'''

"""Create a list of (eigenvalue, eigenvector) tuples and sort them in descending order. Find the explained variance (Percentage of variance attributed by each of the selected components). Higher explained variances indicate higher information % and less information loss."""

# Create a list of (eigenvalue, eigenvector) tuples
eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]
# Sort from high to low
eig_pairs.sort(key = lambda x: x[0], reverse= True)
# Calculation of Explained Variance from the eigenvalues
tot = sum(eig_vals)
var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance
print(len(cum_var_exp))

"""Cumulative Explained Variance is for the array of the variance of the data explained by each of the principal components in the data"""

# PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED 
plt.step(range(6), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='best')

from sklearn.decomposition import PCA # Principal Component Analysis module
#define scaler
scaler = StandardScaler()
#create copy of DataFrame
scaled_df=tempDf.copy()
#created scaled version of DataFrame
scaled_df=pd.DataFrame(scaler.fit_transform(scaled_df), columns=scaled_df.columns)
#define PCA model to use
pca = PCA() 
#Most important statement above - when we are not sure how many components
#fit PCA model to data
pca_fit = pca.fit(scaled_df)

"""Scree plots is a visual way to determine how many of the principal components needs to be retained in analysis"""

PC_values = np.arange(pca.n_components_) + 1
plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.show()

"""The blue line from above graph indicates the cumulative sum. Interpretation - the 1st Principal Component (PC) explains around 55% of variance, 1st and 2nd PCs combined explain 55%+20% = 75% of the variance, 1st, 2nd and 3rd PCs combined together explain 55%+20%+9% = 84% of the variance. Hence, 3 principal components are required."""

#Finding the score that influences the variables
#Reading the columns and their scores
print(tempDf.columns)
pc1 = pca.components_[0]
pc2 = pca.components_[1]
pc3 = pca.components_[2]
print("PC1: ",pc1, end="\n")
print("PC2: ", pc2,end="\n")
print("PC3: ",pc3,end="\n")

"""Applying PCA for 3 PCs."""

from sklearn.decomposition import PCA # Principal Component Analysis module
pca = PCA(n_components=3)
x_4d = pca.fit_transform(X_std)

"""K means clustering is used to compute centroids and repeats until optimal centroid is found; number of clusters formed = K"""

from sklearn.cluster import KMeans # KMeans clustering 
kmeans = KMeans(n_clusters = 3) #Number of clusters needed  = 3
X_clustered = kmeans.fit_predict(x_4d) #Dataset used for clustering is x_4d
LABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b'} #label colors
label_color = [LABEL_COLOR_MAP[l] for l in X_clustered]

"""Create a graph with 3 clusters"""

#Create 2d dataset with 3 blobs
from sklearn.datasets import make_blobs
X, y_true = make_blobs(n_samples=100, centers=3, cluster_std=0.60, random_state=0)
plt.scatter(X[:, 0], X[:, 1], s=20);

# Create a temp dataframe from our PCA projection data "x_4d"
df=pd.DataFrame(PCA(n_components=3).fit_transform(X_std))
df['X_cluster'] = X_clustered
print(df['X_cluster'])

sns.pairplot(df, hue='X_cluster')

"""Feature 2 is more important than feature 0, which is more important than feature 1. i.e., """

x_4d =  PCA(n_components=3).fit(X_std)
X_pc = x_4d.transform(X_std)
n_pcs= x_4d.components_.shape[0] #Number of components
# get the index of the most important feature on EACH component
most_important = [np.abs(x_4d.components_[i]).argmax() for i in range(n_pcs)]
initial_feature_names = ['GDP per capita', 'Social support', 'Healthy life expectancy','Freedom to make life choices', 'Generosity', 'Perceptions of corruption']
#Get the name of the features
most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]
dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}
print(dic.items())

"""Since the order of importance of variables is - PC0> PC1> PC2, hence **GDP per capita is the most important factor in determining the happiness of the country, followed by Perception of corruption and Generosity.**

#Round 3

#1. Given the parameters for any other country, estimate the happiness score.(Example: Multiple Linear Regression model)

Consider all variables to find the equation for happiness score
"""

data.head().columns

x = data[['GDP per capita', 'Social support', 'Healthy life expectancy','Freedom to make life choices', 'Generosity','Perceptions of corruption']]
y = data['Score']

#Splitting the dataset 7:3
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 100)

"""#Applying Linear Model on Training Dataset"""

from sklearn.linear_model import LinearRegression
mlr = LinearRegression()
mlr.fit(x_train, y_train)
print("Intercept: ",mlr.intercept_)
print("Coefficients: ",list(zip(x,mlr.coef_)))

#Create a dataframe - better representation of feature and coefficients
lm = LinearRegression()
lm.fit(x, data.Score)
coef=zip(x.columns, lm.coef_)
#lm.coef_ works only after data has been fit using linear regression
coef_df=pd.DataFrame(list(zip(x.columns, lm.coef_)),columns=["Features","Coefficients"])
coef_df

"""*Why is the coefficient value different from PCA? What is the importance*"""

#Prediction of training set
y_pred_mlr=mlr.predict(x_train)
#Predicted values
print("Prediction for training-set: {}".format(y_pred_mlr))

#Model Evaluation
from sklearn import metrics
meanAbErr = metrics.mean_absolute_error(y_train, y_pred_mlr)
meanSqErr = metrics.mean_squared_error(y_train, y_pred_mlr)
rootMeanSqErr = np.sqrt(metrics.mean_squared_error(y_train, y_pred_mlr))
print('R squared: {:.2f}'.format(mlr.score(x,y)*100))
print('Mean Absolute Error:', meanAbErr)
print('Mean Square Error:', meanSqErr)
print('Root Mean Square Error:', rootMeanSqErr)

"""#Implementing Linear Model on Test Dataset"""

#Fitting the Multiple Linear Regression Model
from sklearn.linear_model import LinearRegression
mlr = LinearRegression()
mlr.fit(x_train, y_train)

#Finding the coefficients and intercept
print("Intercept: ",mlr.intercept_)
print("Coefficients: ",list(zip(x,mlr.coef_)))

#Create a dataframe - better representation of feature and coefficients
lm = LinearRegression()
lm.fit(x, data.Score)
coef=zip(x.columns, lm.coef_)
#lm.coef_ works only after data has been fit using linear regression
coef_df=pd.DataFrame(list(zip(x.columns, lm.coef_)),columns=["Features","Coefficients"])
coef_df

#Prediction of test set
y_pred_mlr=mlr.predict(x_test)
#Predicted values
print("Prediction for test-set: {}".format(y_pred_mlr))

"""Compare Actual with Predicted Value"""

#Actual value and predicted value
mlr_diff = pd.DataFrame({'Actual value': y_test, 'Predicted value': y_pred_mlr})
mlr_diff.head()

"""Evaluate Model"""

#Model Evaluation
from sklearn import metrics
meanAbErr = metrics.mean_absolute_error(y_test, y_pred_mlr)
meanSqErr = metrics.mean_squared_error(y_test, y_pred_mlr)
rootMeanSqErr = np.sqrt(metrics.mean_squared_error(y_test, y_pred_mlr))
print('R squared: {:.2f}'.format(mlr.score(x,y)*100))
print('Mean Absolute Error:', meanAbErr)
print('Mean Square Error:', meanSqErr)
print('Root Mean Square Error:', rootMeanSqErr)

"""**Note that the test data set has r2 score = 77.18% and the training data set has r2 score = 77.18%**"""



"""#2.Find top 5 countries which support a high happiness index in terms of Economy(GDP)and Trust in the government; Here, consider the year 2019

Approach - create dataframe with GDP and trust. Then take top 10 countries of each and display score in graph
"""

data.columns

tempDf=pd.DataFrame(data,columns=['Country or region','Score', 'GDP per capita','Perceptions of corruption'])
tempDf

"""Finding top 5 countries which support a high happiness index in terms of Economy(GDP). Countries with higher GDP per capita are happier."""

gdp = tempDf.sort_values(by=['GDP per capita'], ascending=False).head(5)
gdp

"""Finding top 5 countries which support a high happiness index in Trust in the government. Countries with higher trust in the government have higher happiness index."""

perccorr =  tempDf.sort_values(by=['Perceptions of corruption'], ascending=False).head(5)
perccorr

"""#Model the Happiness Index over a time span of 3 years.
#● Which countries maintain a consistent HI value?
#● Which parameters vary largely with time?
"""

from google.colab import files
uploaded = files.upload()

data17=pd.read_csv("2017.csv")
#insert year in the dataframe
data17['Year']=2017
data18=pd.read_csv("2018.csv")
data18['Year']=2018
data19=pd.read_csv("2019.csv")
data19['Year']=2019

#Drop columns not common
print(data17.columns)
print(data18.columns)
print(data19.columns)

#Drop colns
data17.drop(['Dystopia.Residual','Whisker.high','Whisker.low','Family'],axis=1)
#rename
data17.rename(columns={'Happiness.Score':'Score','Economy..GDP.per.Capita.':'GDP per capita','Health..Life.Expectancy.':'Healthy life expectancy','Trust..Government.Corruption.':'Perceptions of corruption'})
data18.rename(columns={'Country or region':'Country','Freedom to make life choices':'Freedom'})
data19.rename(columns={'Country or region':'Country','Freedom to make life choices':'Freedom'})

#Concat
dataset=[data17,data18,data19]
dataset

